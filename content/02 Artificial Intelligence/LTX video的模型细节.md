---
title: "VAE"
date: 2025-10-29
draft: false
---

# VAE
![[Pasted image 20250812162001.png]]


LTX-Video的VAE并非传统设计，而是为高效视频生成做了针对性优化，核心目标是在**高压缩率**下保持视频质量和生成速度，具体设计包括：  

#### 1. 极高的压缩效率：1:192压缩比与时空下采样  
传统视频生成模型的VAE压缩率通常在1:48到1:96，而LTX-Video的VAE通过**时空下采样**实现了1:192的压缩比（即输入192个像素仅需1个潜在空间“ token ”表示）。具体来说，它对视频的空间维度（宽、高）下采样32倍，时间维度（帧数）下采样8倍，最终每个token对应32×32×8像素的视频片段。  

这种高压缩的好处是：大大减少输入到Transformer的token数量，让模型能高效处理全时空自注意力（计算量与token数量平方相关），为“实时生成”奠定基础。  


#### 2. 关键操作迁移：把“分块（patchifying）”移到VAE输入  
传统模型中，视频先经VAE压缩到潜在空间，再由Transformer对潜在空间的特征进行“分块”（切成小patch作为输入token）。而LTX-Video直接在VAE的编码器输入阶段完成分块，跳过了Transformer的分块步骤。  

这一改动的作用是：进一步减少token数量（无需在潜在空间二次分块），同时让VAE直接学习“如何压缩成适合Transformer处理的token”，优化VAE与Transformer的协同效率。  


#### 3. 解码器的双重任务：从潜在空间到像素+最后去噪  
传统VAE的解码器仅负责“从潜在空间重建像素”，而LTX-Video的解码器还承担了“最后一步去噪”任务：在将潜在空间表示转换为像素时，同步消除残留噪声，直接输出清晰的视频帧。  

这解决了高压缩率的固有问题：高压缩会丢失部分高频细节（如纹理、边缘），解码器通过“生成”而非单纯“重建”这些细节，在无需额外上采样模块的情况下，保留了视频的精细度🔶1-26🔶。  


#### 4. 优化潜在空间质量的辅助设计  
为避免高压缩导致的信息丢失或重建模糊，VAE还引入了多项改进：  
- **共享扩散目标**：让解码器作为扩散模型，直接从带噪声的潜在空间生成干净像素，而非依赖Transformer在潜在空间完成所有去噪🔶1-60🔶；  
- **重建GAN（rGAN）**：传统GAN让判别器区分“真实 vs 生成”，而rGAN让判别器对比“同一视频的原始帧 vs 重建帧”，更精准引导解码器还原细节；  
- **多层噪声注入**：在解码器多个层注入噪声，增加高频细节的多样性（类似StyleGAN的风格控制）；  
- **均匀log方差**：避免传统KL损失导致的“部分潜在通道被浪费”，让所有通道均匀参与信息编码。  


### 三、图3的含义：潜在空间的“冗余性”随训练变化  
图3展示了LTX-Video的VAE在训练过程中，**潜在空间的信息利用效率如何提升**，具体包括三个子图：  

#### （a）潜在通道的累积解释方差  
纵轴是“累积解释方差”（越接近1，说明该通道集合能表示越多原始信息），横轴是“潜在通道数量”。图中曲线随训练进度（2%到100%）变化：  
- 训练初期（2%）：少数通道（如前20个）就贡献了大部分方差（曲线陡峭），说明通道利用集中，存在冗余；  
- 训练后期（100%）：曲线平缓上升，更多通道均匀贡献方差，说明模型学会了用所有通道高效编码信息，冗余减少。  


#### （b）和（c）潜在通道的自相关矩阵  
矩阵中“非对角线元素”的数值表示两个通道的相关性（值越高，冗余越高）：  
- （b）训练4%时：非对角线元素值较高，说明通道间信息重叠多，冗余严重；  
- （c）训练结束时：非对角线元素接近0，说明通道间几乎独立，每个通道承担独特的信息编码任务，冗余大幅降低。  


### 总结  
LTX-Video的VAE通过“高压缩率设计”“操作迁移”“解码器双重任务”等创新，在大幅降低计算成本的同时，保证了视频质量；而图3则验证了其潜在空间随训练优化，通道利用更高效、冗余更少，为后续Transformer的快速处理提供了高质量的“输入密码”。


# Image Condition
![[Pasted image 20250811103938.png]]

LTX-Video实现“从给定第一帧生成视频”（即image-to-video）的核心是通过**基于扩散时间步（timestep）的逐标记（per-token）条件控制机制**，无需特殊标记或专门训练的模型，具体实现可分为训练阶段和推理阶段两部分，详细如下：


### 一、核心改进：突破传统模型的时间步限制
传统扩散模型（如DiT [9]、Pixart-α [8]）中，所有标记（tokens）会被注入相同的时间步嵌入，并按统一噪声水平添加噪声，无法区分“需要参考的帧”和“需要生成的帧”。  
而LTX-Video放松了这一限制，允许**每个标记拥有独立的时间步和对应的噪声水平**，通过时间步的差异来区分“作为条件的第一帧”和“待生成的后续内容”，这是实现从第一帧生成视频的基础。


### 二、训练阶段：让模型学习“第一帧作为条件”的信号
为了使模型学会利用第一帧作为生成依据，训练时会针对性地处理第一帧对应的标记：  
- 偶尔将第一帧对应的标记的时间步设为**小随机值**（而非统一的大时间步）；  
- 按该小时间步对应的噪声水平对这些标记添加噪声（噪声量远低于其他标记）。  

通过这种设置，模型会逐渐学习到：“时间步较小、噪声水平较低的标记”是需要参考的“条件信号”（即第一帧），并基于此生成后续连贯的内容。


### 三、推理阶段：从第一帧生成视频的具体流程
推理阶段即实际从给定第一帧生成视频的过程，分为以下步骤：  
1. **编码条件图像（第一帧）**：将作为条件的第一帧通过模型的**因果VAE编码器**进行处理，生成一个时间维度为1的潜在张量（latent tensor）——这一步将像素空间的第一帧转换为模型可处理的潜变量形式。  
2. **构建初始标记序列**：将编码得到的潜在张量（对应第一帧）与**随机噪声潜变量**（用于生成后续帧）进行拼接，再展平为完整的初始标记集（tokens）。其中，拼接操作确保第一帧的信息被纳入生成流程，作为后续内容的参考基础。  
3. **设置逐标记的时间步**：为初始标记集中的每个标记分配去噪时间步：  
   - 对应第一帧的“条件标记”：设置为小值$t_c$（例如$t_c=0$），表示这些标记噪声极低（甚至无噪声），需保留其原始信息作为生成参考；  
   - 对应后续帧的“生成标记”：设置为$t=1$，表示这些标记初始为纯噪声，需要模型完全基于第一帧的条件生成内容。  


### 核心逻辑总结
LTX-Video通过**“逐标记的时间步和噪声水平控制”**，将第一帧的信息以“低时间步、低噪声”的形式嵌入生成流程，使模型能自动识别并基于该帧生成连贯的后续内容。这种机制无需特殊标记或额外训练模型，仅通过调整扩散过程的原生变量（时间步）实现条件控制，既简化了设计，又保证了生成的时序一致性。