在 PyTorch 或深度学习框架中，**计算图（Computational Graph）** 是描述数学运算和数据流动的**有向无环图（DAG）**，它是自动微分（Autograd）的核心机制。以下是关键概念和作用的详解：

---

### 1. **计算图的结构**
- **节点（Node）**：表示张量（Tensor）或运算（如加法、矩阵乘法）。
- **边（Edge）**：表示张量之间的依赖关系（数据流动方向）。
- **动态性**：PyTorch 的计算图是动态构建的（每次前向传播会重新生成）。

#### 示例代码：
```python
import torch

x = torch.tensor(2.0, requires_grad=True)
y = x ** 2 + 3  # 计算图自动构建
```
对应的计算图：
```
x (Tensor) → Square → Add(3) → y (Tensor)
```

---

### 2. **计算图的核心作用**
#### （1）**自动微分（Autograd）**
- 计算图记录所有操作的顺序和依赖关系，使得反向传播时能自动计算梯度。
- 例如，对 `y` 调用 `y.backward()` 会沿计算图回溯，计算 `x.grad`（即 `dy/dx`）。

#### （2）**梯度计算原理**
- 反向传播时，框架会从输出节点开始，按链式法则逐层计算梯度。
- 例如上述代码中：
  - `dy/dx = d(x² + 3)/dx = 2x` → 当 `x=2` 时，梯度值为 `4`。

---

### 3. **计算图的动态特性**
- **即时构建（Define-by-Run）**：PyTorch 的计算图在前向传播时实时构建，每次迭代可以不同（与 TensorFlow 的静态图不同）。
- **灵活性**：支持条件分支、循环等动态控制流。

#### 动态图示例：
```python
if x > 0:
    y = x * 2
else:
    y = x + 1
# 计算图会根据 x 的值动态生成不同路径
```

---

### 4. **计算图的生命周期**
1. **前向传播（Forward）**：构建计算图，记录操作。
2. **反向传播（Backward）**：根据计算图计算梯度。
3. **销毁**：默认情况下，计算图在一次反向传播后会自动释放（除非指定 `retain_graph=True`）。

---

### 5. **与 `@torch.no_grad()` 的关系**
- 当使用 `@torch.no_grad()` 时，PyTorch **不会构建计算图**，因此：
  - 不记录操作依赖关系。
  - 无法执行反向传播（节省内存和计算资源）。

---

### 6. **可视化工具**
- **Torchviz**：通过 `torchviz.make_dot` 可视化计算图。
  ```python
  from torchviz import make_dot
  make_dot(y, params={'x': x}).render("graph")  # 生成计算图图片
  ```

---

### 总结
- **本质**：计算图是深度学习框架中实现自动微分的核心数据结构。
- **PyTorch 特点**：动态图机制提供灵活性，适合研究调试；静态图（如 TorchScript）则适合部署优化。
- **性能影响**：计算图构建会占用内存，因此在推理时应使用 `@torch.no_grad()` 避免冗余开销。