---
title: "focal loss"
date: 2025-08-07
draft: false
---

Focal Loss 是一种为了解决类别不平衡问题而提出的损失函数，最早在《Focal Loss for Dense Object Detection》这篇论文中提出，主要应用于**目标检测**任务，尤其是在处理**类别不平衡**的情况下表现出色。==Focal Loss 对易分类样本施加较小的权重，增加难分类样本的权重，从而帮助模型专注于困难的样本，减轻简单样本的影响。==

## Focal Loss 的数学形式
Focal Loss 是在交叉熵损失的基础上进行改进的。其公式如下：
$$

FL(p_t) = -\alpha_t (1 - p_t)^\gamma \log(p_t)

$$

其中：
- $p_t$ 是模型对当前样本的预测概率，表示模型对于正类（$y=1$）或者负类（$y=0$）的预测置信度。
- 对于正类：$p_t = p$
- 对于负类：$p_t = 1 - p$
- $\alpha_t$ 是一个调节参数，用于平衡类别不平衡。如果正负样本的比例不平衡，可以通过调整$\alpha_t$来调整。
- $\gamma$ 是一个调节焦点的参数。它控制了难易样本的区别。$\gamma > 0$ 会使得损失函数更加关注难分类的样本。

  

**公式解析：**

• 当 $p_t$ 较大时（即样本很容易被分类），$FL(p_t)$ 将变得较小，因为 $(1 - p_t)^\gamma$ 会接近于0，从而降低了简单样本对总损失的贡献。

• 当 $p_t$ 较小时（即样本难以分类），$FL(p_t)$ 会增大，因为 $(1 - p_t)^\gamma$ 对于较小的 $p_t$ 会变得较大，从而提升了困难样本对总损失的影响。

  

**关键参数**

1. **$\alpha_t$**: 用于平衡类别不平衡。如果在分类中正负样本比例严重不平衡（例如正样本远小于负样本），可以通过调整 $\alpha_t$，例如让正样本的 $\alpha_t$ 更大，来加大正样本的权重。通常，$\alpha_t$ 是一个常数，也可以根据类别不平衡的程度动态调整。

2. **$\gamma$**: 焦点参数，通常设置为 2，目的是加强对困难样本的关注。$\gamma$ 的值越大，模型对于易分类样本的忽视程度越高，更多地关注难分类样本。

  

**Focal Loss 的特点**

1. **解决类别不平衡问题**：传统的交叉熵损失容易受到数量较多的负样本影响，从而使得模型偏向于负类。而 Focal Loss 通过调整易分类样本的损失权重，增强了对难分类样本的关注，减轻了类别不平衡的影响。

2. **动态调节焦点**：随着训练的进行，容易分类的样本的损失逐渐减小，模型更多地关注难分类的样本。这样，可以提高模型在困难样本上的分类精度。

  

**Focal Loss 的应用**

• **目标检测**：Focal Loss 最初是为了改进 Faster R-CNN 等目标检测算法中的类别不平衡问题而提出的，尤其在大规模数据集上，正负样本比例极其不平衡时，能够有效提高检测性能。

• **图像分割**：在处理类别不平衡的分割任务时，Focal Loss 也有一定应用，帮助关注难以分割的区域。

  

**举个例子：**

  

假设我们在进行二分类任务，模型的输出为 $p$（预测为正类的概率），实际标签 $y$ 为 1 或 0。我们使用标准[[交叉熵损失]]计算：

$$

\text{CE}(p) = - y \log(p) - (1 - y) \log(1 - p)

$$

  

当使用 Focal Loss 时，对于容易分类的样本（例如 $p$ 很大或很小的样本），我们希望它们对总损失的贡献较小。这时，Focal Loss 会将这些样本的损失衰减，从而减小它们对模型训练的影响，集中精力训练那些困难的样本。

  

如果 $\gamma = 2$ 和 $\alpha_t = 0.25$，对于一个难分类的正类样本（$p = 0.2$）：

$$

FL(p) = -0.25 \times (1 - 0.2)^2 \log(0.2) = 0.25 \times 0.64 \times 1.609 = 0.257

$$

而对于一个容易分类的正类样本（$p = 0.8$）：

$$

FL(p) = -0.25 \times (1 - 0.8)^2 \log(0.8) = 0.25 \times 0.04 \times 0.223 = 0.002

$$

这样，难分类的样本贡献的损失较大，易分类样本的损失被有效衰减。

  

**总结：**

  

Focal Loss 是一种针对类别不平衡问题设计的损失函数，通过调节易分类样本和难分类样本的损失权重，使得模型在训练过程中更加关注难分类的样本，减少易分类样本的干扰，尤其适用于目标检测等任务中正负样本不平衡的场景。