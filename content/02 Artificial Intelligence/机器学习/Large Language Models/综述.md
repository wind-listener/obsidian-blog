---
title: "NLP任务技术迭代"
date: 2025-08-07
draft: false
---


> 自从上世纪50年代“图灵测试”被提出，人们一直试图让机器获得智能。自然语言处理，Nature Language Process，NLP领域一直是关注的重点，技术也不断迭代。从开始的基于统计、神经网络，到现在主流的基于Transformer的GPT框架，模型能力越来越强，实现了令人振奋的效果和应用。
> 大语言模型（LLM）是指包含数千亿或更多参数的Transformer语言模型，通过在大规模文本数据上进行训练，展现出理解自然语言和解决复杂任务的强大能力。自20世纪50年代图灵测试提出以来，语言智能的研究不断推进，语言建模在过去的二十年中逐步发展为神经语言模型，预训练语言模型（PLM）成为主流。

# NLP任务技术迭代
![[Pasted image 20240516180632.png]]
## 统计语言模型 Statistical LM
> 词袋模型、[[马尔可夫链]]、维数灾难、n元语言模型

## 神经语言模型 Neural LM
> RNN、word2vec

## 预训练语言模型 Pre-trained LM
> biLSTM、BERT、Pre-trained + Fine-tune



## 大语言模型 LLM
>涌现能力、AGI(Artificial General Intelligence)通用人工智能、未知领域

思路与PLM，在本质上是一样的，但是随着模型体量的爆炸增长，LLM涌现出了许多PLM不具备的能力。

# LLM 概述
## 背景知识
1. **扩展法则**：通过扩展模型规模、数据规模和总计算量，可以大幅提高LLM的性能。代表性的扩展法则包括KM扩展法则和Chinchilla扩展法则。
2. **涌现能力**：LLM在扩展到一定规模后，展现出小型PLM所不具备的特殊能力，如上下文学习（In-Context Learning，ICL）、指令遵循和逐步推理。
3. **关键技术**：LLM的成功依赖于扩展、训练、能力引导、对齐微调和工具操作等技术。

## GPT系列模型的演进
**代表性模型**：
1. **GPT系列**：包括GPT-1、GPT-2、GPT-3、GPT-3.5和GPT-4。GPT-3引入了上下文学习的概念，GPT-3.5通过使用代码数据进行训练，增强了推理能力，GPT-4则扩展到多模态信号。
2. **PaLM**：通过增加模型规模和训练数据量，显著提升了模型能力。
3. **ChatGPT**：基于GPT模型开发，专门优化对话能力，通过插件机制扩展功能。


## 目前LLM 的完整流程
1. **预训练**：通过在大规模语料库上进行预训练，捕捉上下文感知的词表示，然后根据特定的下游任务微调模型。
2. **适配微调**：通过指令微调和基于人类反馈的强化学习（RLHF）方法，提升模型在具体任务上的表现，并确保与人类价值观对齐。
3. **使用**：通过设计合适的任务指令或具体的ICL策略，激发LLM的潜在能力，解决各种下游任务。
4. **能力评估**：通过一系列评估基准和实验，验证LLM在复杂任务上的性能。

# LLM 的 Pre-trained
# 预训练语料库来源
1. 通用文本数据： 网页、书籍、对话文本等
2. 专用文本数据：多语言文本、科学文本、代码
## 数据预处理
典型流程
![[Pasted image 20240518174839.png]] 
1. 质量过滤
	1. 基于分类器的方法
	2. 基于启发器的方法
		一般规则：基于语言、度量、统计、关键词
2. 去重
3. 隐私去除
4. 分词

# LLM 的 Fine-tuning
未来发展方向：
1. **LLM的基本原理探索**：研究LLM涌现能力的关键因素，揭示其“秘密”。
2. **训练策略优化**：由于计算资源需求巨大，需要优化训练策略，解决工程问题。
3. **对齐与安全**：确保LLM生成内容与人类价值观对齐，消除潜在风险，促进安全部署。

**总结**：LLM的技术发展对AI领域产生了重大影响，推动了通用人工智能（AGI）的研究和应用。通过综述LLM的背景、主要发现、技术发展和未来方向，本文为研究人员和工程师提供了全面的参考。

希望这段总结能帮助你更好地理解大语言模型综述的核心内容和技术发展。