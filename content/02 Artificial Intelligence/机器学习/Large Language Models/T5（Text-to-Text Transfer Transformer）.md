---
title: "T5（Text-to-Text Transfer Transformer）"
date: 2025-08-16
draft: false
---

T5（Text-to-Text Transfer Transformer）是 Google Research 开发的一个通用文本生成和处理模型，首次提出于论文 [“Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer”](https://arxiv.org/abs/1910.10683)。

T5 模型通过统一的***文本到文本***框架处理各类NLP任务，其编码器（Encoder）是理解输入文本的核心组件。以下从输入到输出详细解析编码流程，包括矩阵变换原理及意义，结合技术细节分阶段说明：

---

### **1. 输入预处理与向量化**
- **Tokenization**：  
  输入文本（如 `"translate English to German: The house is wonderful."`）通过SentencePiece子词分词器切分为子词序列（如 `["▁translate", "▁English", "▁to", "▁German", ":", "▁The", "▁house", "▁is", "▁wonderful", "."]`），并添加结束符 `</s>`。分词后序列长度为 `L`（示例中 `L=10`）。  
- **数值映射**：  
  每个子词转换为词表ID（词表大小 `V=32,128`），生成整数序列 `[2163, 8, 19, 1254, ...]`，形状为 `(1, L)`（批大小为1时）。  
- **词嵌入（Embedding）**：  
  通过嵌入矩阵 `W_embed ∈ R^(V×d)` 将ID映射为 `d` 维向量（`d` 为隐藏层维度）。输出张量形状：`(1, L, d)`。  
  **意义**：将离散符号转化为连续向量，保留语义信息，为后续神经网络处理奠基。

---

### **2. 位置编码（Positional Encoding）**
- **相对位置编码**：  
  T5摒弃绝对位置编码，采用**标量形式的相对位置编码**。计算查询（Query）和键（Key）的相对位置差 `δ = i - j`，映射到32个桶（buckets）：  
  - 小偏移（`|δ|≤8`）直接映射到桶 `[0, 15]`  
  - 大偏移（`|δ|>8`）按对数分段映射到桶 `[16, 31]`  
  每个注意力头独立学习位置嵌入向量 `r_k ∈ R^32`，通过查表获取标量 `s_{ij} = r_{b(δ)}`，加到注意力分数 `a_{ij}` 上。  
  **意义**：降低长序列计算复杂度（从 `O(L²d)` 降至 `O(L²)`），增强模型对位置关系的泛化能力。

---

### **3. 编码器堆栈（Encoder Stack）**
编码器由 `N` 个相同层堆叠（如T5-Base有12层），每层包含以下子模块：  

#### **(a) 多头自注意力（Multi-Head Self-Attention）**
- **输入**：上一层的输出 `X ∈ R^(L×d)`  
- **线性变换**：  
  生成Query、Key、Value矩阵：  
  ```
  Q = X · W_Q,  K = X · W_K,  V = X · W_V  
  （W_Q, W_K, W_V ∈ R^(d×d_k)）
  ```
  `d_k = d / h`（`h` 为注意力头数，如T5-Base中 `h=12`）。输出分头张量形状：`(h, L, d_k)`  
- **注意力分数**：  
  `Attention = softmax( (Q·K^⊤)/√d_k + S ) · V`，其中 `S` 为相对位置编码标量矩阵。  
  **意义**：捕获序列内任意词对间的依赖关系（如 `"house"` 与 `"wonderful"` 的修饰关系）。

#### **(b) 残差连接与RMSNorm**
- **残差连接**：  
  注意力输出 `Y_attn` 与输入 `X` 相加： `Z = X + Y_attn`  
- **RMSNorm**：  
  替代LayerNorm，无偏置项，计算效率更高：  
  ```
  RMSNorm(Z) = Z / √(mean(Z²) + ε) · γ  
  （γ ∈ R^d 为可学习缩放参数）
  ```
  **意义**：缓解梯度消失，加速收敛。

#### **(c) 前馈神经网络（FFN）**
- **两层全连接**：  
  `FFN(Z) = gelu(Z · W_1 + b_1) · W_2 + b_2`  
  （`W_1 ∈ R^(d×d_ff)`, `W_2 ∈ R^(d_ff×d)`，T5-Base中 `d_ff=2048`）  
- **二次残差与归一化**：  
  输出再次经过 `Z' = Z + FFN(Z)` 和 `RMSNorm(Z')`。  
  **意义**：引入非线性变换，增强特征表达能力。

---

### **4. 编码器输出**
- **最终隐藏状态**：  
  `N` 层堆叠后输出 `H_enc ∈ R^(L×d)`，包含输入序列的上下文信息（如 `"The house is wonderful"` 整体语义）。  
- **维度不变性**：  
  输入输出形状均为 `(1, L, d)`，信息在传递中逐步深化而非改变维度。  
  **意义**：为解码器提供全局语义表示，支撑生成任务。

---

### **5. 与解码器的交互**
编码器输出 `H_enc` 作为解码器的**键值对（Key-Value）**：  
- 解码器每层通过**交叉注意力**访问 `H_enc`：  
  ```
  Q_dec = Decoder_State · W_Q^{cross}  
  K_enc = H_enc · W_K^{cross},  V_enc = H_enc · W_V^{cross}  
  ```
- **动态信息检索**：  
  解码器生成每个token时（如德语词 `"Das"`），通过 `Q_dec` 从 `H_enc` 中检索相关部分（如聚焦输入中的 `"The house"`）。  
  **意义**：实现输入与输出的动态对齐，解决长距离依赖问题。

---

### **关键矩阵维度总结**
| **处理阶段**          | **输入形状**       | **输出形状**       | **参数示例（T5-Base）**       |
|------------------------|--------------------|--------------------|-------------------------------|
| Tokenization & Embedding | 文本序列           | (1, L)             | `L=10`（序列长度）           |
| 词嵌入                 | (1, L)            | (1, L, d)          | `d=768`                      |
| 自注意力分头           | (1, L, d)         | (h, L, d_k)        | `h=12`, `d_k=64`             |
| 自注意力合并           | (h, L, d_k)       | (1, L, d)          | -                            |
| FFN                    | (1, L, d)         | (1, L, d)          | `W_1: (768, 2048)`, `W_2: (2048, 768)` |
| 编码器输出             | (1, L, d)         | (1, L, d)          | -                            |

---

### **统一框架的意义**
T5编码器的设计充分体现了其**文本到文本大一统思想**：  
1. **任务无关性**：无论输入是翻译指令、分类前缀还是问答上下文，均通过同一编码器处理。  
2. **预训练与微调一致性**：Span-Corruption预训练（如遮蔽连续词段 `"is wonderful"` → 预测 `<extra_id_0>`）与下游任务共享编码器架构。  
3. **可扩展性**：通过调整层数（`N`）、隐藏维度（`d`）、头数（`h`）等参数，灵活适配不同算力需求（如T5-Small到T5-11B）。  

此设计使T5成为通用文本理解与生成的强有力工具，为后续多模态模型（如Imagen的文本编码器）奠定基础。

# 推荐博客
- [T5 模型：NLP Text-to-Text 预训练模型超大规模探索](https://zhuanlan.zhihu.com/p/88438851)
- [Hugging Face 文档-T5](https://hugging-face.cn/docs/transformers/model_doc/t5)
