---
title: "æŸå¤±å‡½æ•°"
date: 2025-08-07
draft: false
---

åœ¨ PyTorch ä¸­ï¼Œnn æä¾›äº†å¤šç§æŸå¤±å‡½æ•° (criterion)ï¼Œç”¨äºä¸åŒçš„ä»»åŠ¡ã€‚å¯¹äº **äºŒåˆ†ç±»ä»»åŠ¡**ï¼ˆbinary classificationï¼‰ï¼Œåˆé€‚çš„æŸå¤±å‡½æ•°åŒ…æ‹¬ï¼š

  

**1. é€‚ç”¨äºäºŒåˆ†ç±»ä»»åŠ¡**

  

**æŸå¤±å‡½æ•°** **è¯´æ˜** **YAML é…ç½®**

BCELoss é€‚ç”¨äºäºŒåˆ†ç±»ä»»åŠ¡ï¼Œé…åˆ Sigmoid è¾“å‡ºä½¿ç”¨ loss: "BCELoss"

BCEWithLogitsLoss é€‚ç”¨äºäºŒåˆ†ç±»ï¼Œå†…éƒ¨åŒ…å« Sigmoidï¼Œå¯ç›´æ¥ç”¨äº logits è¾“å‡º loss: "BCEWithLogitsLoss"

CrossEntropyLoss é€‚ç”¨äºå¤šåˆ†ç±»ä»»åŠ¡ï¼ˆå¦‚ [0,1] one-hot æ ‡ç­¾ï¼‰ loss: "CrossEntropyLoss"

  

ğŸ’¡ **æ¨è**ï¼š

â€¢ å¦‚æœ MLPClassifier çš„æœ€åä¸€å±‚æ˜¯ Sigmoid()ï¼Œè¯·ä½¿ç”¨ BCELossã€‚

â€¢ å¦‚æœæœ€åä¸€å±‚ **ä¸åŠ ** Sigmoid()ï¼Œè¯·æ”¹ç”¨ BCEWithLogitsLossï¼ˆæ¨èï¼‰ã€‚

â€¢ CrossEntropyLoss é€‚ç”¨äº **å¤šåˆ†ç±»ä»»åŠ¡**ï¼ˆæ¯”å¦‚ [0, 1] çš„ one-hot ç¼–ç ï¼‰ã€‚

  

**2. é€‚ç”¨äºå…¶ä»–ä»»åŠ¡ï¼ˆæ‰©å±•ç”¨ï¼‰**

  

**æŸå¤±å‡½æ•°** **é€‚ç”¨æƒ…å†µ** **YAML é…ç½®**

MSELoss é€‚ç”¨äºå›å½’ä»»åŠ¡ï¼Œå¦‚è¾“å‡º **ç½®ä¿¡åº¦/è¯„åˆ†** loss: "MSELoss"

L1Loss é€‚ç”¨äº **ç¨€ç–æ•°æ®/ç¨³å¥æŸå¤±**ï¼Œå¦‚ **å¼‚å¸¸æ£€æµ‹** loss: "L1Loss"

SmoothL1Loss MSELoss + L1Loss ç»“åˆï¼Œé€‚ç”¨äºé²æ£’å›å½’ loss: "SmoothL1Loss"

HingeEmbeddingLoss é€‚ç”¨äºå¯¹æ¯”å­¦ä¹ ï¼ˆ-1/1 æ ‡ç­¾ï¼‰ loss: "HingeEmbeddingLoss"

CosineEmbeddingLoss é€‚ç”¨äºåº¦é‡å­¦ä¹ ï¼Œè®¡ç®—å‘é‡ç›¸ä¼¼åº¦ loss: "CosineEmbeddingLoss"

  

**3. å¦‚ä½•é€‰æ‹©ï¼Ÿ**

  

**âœ… æ¨è BCEWithLogitsLoss**

  

loss: "BCEWithLogitsLoss"

  

â€¢ **ç¡®ä¿** MLPClassifier **çš„æœ€åä¸€å±‚å»æ‰** Sigmoid()ï¼Œæ”¹ä¸ºï¼š

  

layers.append(nn.Linear(hidden_sizes[-1], 1))Â  # ç›´æ¥è¾“å‡º logits

  

  

  

**âœ… ä½¿ç”¨ BCELoss**

  

loss: "BCELoss"

  

â€¢ **ç¡®ä¿** MLPClassifier **æœ€åä¸€å±‚æ˜¯** Sigmoid()ï¼š

  

layers.append(nn.Linear(hidden_sizes[-1], 1))

layers.append(nn.Sigmoid())

  

  

  

**âœ… é€‚ç”¨äºç½®ä¿¡åº¦è¯„åˆ†/å›å½’**

  

loss: "MSELoss"

  

â€¢ é€‚ç”¨äº **æ¨¡å‹è¾“å‡ºå›å½’å€¼**ï¼Œå¦‚è§†é¢‘è´¨é‡è¯„åˆ†ä»»åŠ¡ã€‚

  

**4. config.yaml ç¤ºä¾‹**

  

- name: "mlp_bce"

Â  batch_size: 32

Â  epochs: 50

Â  lr: 1e-4

Â  hidden_sizes: [2048, 512, 128]

Â  dropout: 0.3

Â  optimizer: "Adam"

Â  loss: "BCEWithLogitsLoss"Â  # æ¨è

Â  save_interval: 5

  

- name: "mlp_mse"

Â  batch_size: 16

Â  epochs: 30

Â  lr: 5e-5

Â  hidden_sizes: [1024, 256, 64]

Â  dropout: 0.2

Â  optimizer: "SGD"

Â  loss: "MSELoss"Â  # é€‚ç”¨äºå›å½’

Â  save_interval: 10

  

**5. ä»£ç å¦‚ä½•å¤„ç†æŸå¤±å‡½æ•°ï¼Ÿ**

  

criterion = getattr(nn, config["loss"])()

  

âœ… **å¯ä»¥ä»** config.yaml **åŠ¨æ€é€‰æ‹©** lossï¼Œæ— éœ€æ‰‹åŠ¨æ”¹ä»£ç ã€‚

  

**æ€»ç»“**

â€¢ **äºŒåˆ†ç±»ä»»åŠ¡**

â€¢ âœ… **æ¨è** BCEWithLogitsLossï¼ˆå»æ‰ Sigmoid()ï¼‰

â€¢ âœ… **ä¹Ÿå¯ç”¨** BCELossï¼ˆä¿ç•™ Sigmoid()ï¼‰

â€¢ **å›å½’/è¯„åˆ†**

â€¢ âœ… MSELoss / L1Loss

â€¢ **å¯¹æ¯”å­¦ä¹ **

â€¢ âœ… HingeEmbeddingLoss / CosineEmbeddingLoss

  

**è¿™æ ·ï¼Œä½ çš„** train.py **ä»£ç å¯ä»¥å…¼å®¹å¤šä¸ªä»»åŠ¡ï¼Œå˜å¾—æ›´çµæ´»ï¼ğŸš€**