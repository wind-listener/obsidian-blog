---
obsidianUIMode: preview
---
# HDF5è¯¦è§£ï¼šé«˜æ•ˆç®¡ç†å¤§è§„æ¨¡æ•°æ®çš„ç»ˆææŒ‡å—

## 1. ä»€ä¹ˆæ˜¯HDF5ï¼Ÿä¸ºä»€ä¹ˆå®ƒå¦‚æ­¤é‡è¦ï¼Ÿ

HDF5ï¼ˆHierarchical Data Format version 5ï¼‰æ˜¯ä¸€ç§ç”¨äºå­˜å‚¨å’Œç»„ç»‡å¤§é‡æ•°æ®çš„**è·¨å¹³å°æ–‡ä»¶æ ¼å¼**ï¼Œç”±ç¾å›½å›½å®¶è¶…çº§è®¡ç®—åº”ç”¨ä¸­å¿ƒï¼ˆNCSAï¼‰å¼€å‘ã€‚å®ƒå·²ç»æˆä¸ºç§‘å­¦è®¡ç®—ã€é‡‘èåˆ†æå’Œæœºå™¨å­¦ä¹ ç­‰é¢†åŸŸå¤„ç†å¤§è§„æ¨¡æ•°æ®çš„**é¦–é€‰æ ‡å‡†**ã€‚

### 1.1 HDF5çš„æ ¸å¿ƒä¼˜åŠ¿

HDF5ä¹‹æ‰€ä»¥å¤‡å—é’çï¼Œä¸»è¦æºäºå…¶å‡ å¤§æ ¸å¿ƒä¼˜åŠ¿ï¼š

- **åˆ†å±‚ç»“æ„**ï¼šç±»ä¼¼äºæ–‡ä»¶ç³»ç»Ÿçš„æ–‡ä»¶å¤¹ç»“æ„ï¼Œå…è®¸ç”¨æˆ·ä»¥æ ‘å½¢æ–¹å¼ç»„ç»‡æ•°æ®
- **é«˜æ•ˆå‹ç¼©**ï¼šæ”¯æŒgzipã€bloscç­‰å¤šç§å‹ç¼©ç®—æ³•ï¼Œæ˜¾è‘—å‡å°‘å­˜å‚¨ç©ºé—´å ç”¨
- **è·¨å¹³å°å…¼å®¹**ï¼šå¯åœ¨Windowsã€Linuxã€macOSç­‰ç³»ç»Ÿé—´æ— ç¼å…±äº«
- **é«˜æ€§èƒ½I/O**ï¼šé’ˆå¯¹å¤§å‹æ•°æ®é›†ä¼˜åŒ–ï¼Œæä¾›å¿«é€Ÿçš„æ•°æ®è®¿é—®èƒ½åŠ›
- **è‡ªæè¿°æ€§**ï¼šæ–‡ä»¶åŒ…å«å…ƒæ•°æ®ï¼Œä¾¿äºé•¿æœŸä¿å­˜å’Œæ•°æ®ç†è§£

### 1.2 HDF5çš„å…¸å‹åº”ç”¨åœºæ™¯

HDF5å¹¿æ³›åº”ç”¨äºæ•°æ®å¯†é›†å‹é¢†åŸŸï¼ŒåŒ…æ‹¬ï¼š
- ç§‘å­¦è®¡ç®—ï¼ˆç‰©ç†æ¨¡æ‹Ÿã€æ°”å€™å»ºæ¨¡ï¼‰
- é‡‘èé«˜é¢‘äº¤æ˜“æ•°æ®å­˜å‚¨
- æœºå™¨å­¦ä¹ æ¨¡å‹å’Œæ•°æ®é›†ç®¡ç†
- åŒ»ç–—å½±åƒæ•°æ®å­˜å‚¨
- é¥æ„Ÿæ•°æ®å’Œå«æ˜Ÿå›¾åƒå¤„ç†

## 2. HDF5æ–‡ä»¶ç»“æ„è§£æ

ç†è§£HDF5çš„æ–‡ä»¶ç»“æ„æ˜¯æŒæ¡å…¶ç”¨æ³•çš„å…³é”®ã€‚HDF5æ–‡ä»¶é‡‡ç”¨å±‚æ¬¡åŒ–ç»„ç»‡æ–¹å¼ï¼ŒåŒ…å«å››ç§æ ¸å¿ƒå¯¹è±¡ï¼š

### 2.1 æ ¸å¿ƒç»„ä»¶

```python
HDF5æ–‡ä»¶ç»“æ„ç¤ºä¾‹ï¼š
/ï¼ˆæ ¹ç»„ï¼‰
â”œâ”€â”€ group1ï¼ˆç»„ï¼‰
â”‚   â”œâ”€â”€ dataset1ï¼ˆæ•°æ®é›†ï¼‰ï¼šå®é™…å­˜å‚¨çš„æ•°æ®æ•°ç»„
â”‚   â””â”€â”€ attribute1ï¼ˆå±æ€§ï¼‰ï¼šæè¿°æ€§å…ƒæ•°æ®
â”œâ”€â”€ group2
â”‚   â”œâ”€â”€ subgroup1
â”‚   â””â”€â”€ dataset2
â””â”€â”€ dataset3
```

**æ–‡ä»¶(File)**ï¼šHDF5çš„æœ€é«˜å±‚æ¬¡ï¼Œæ‰€æœ‰æ•°æ®éƒ½å­˜å‚¨åœ¨å•ä¸ªæ–‡ä»¶ä¸­ï¼Œå…·æœ‰è‡ªåŒ…å«ç‰¹æ€§ã€‚

**ç»„(Group)**ï¼šç±»ä¼¼äºæ–‡ä»¶ç³»ç»Ÿä¸­çš„æ–‡ä»¶å¤¹ï¼Œç”¨äºç»„ç»‡å’Œç®¡ç†æ•°æ®é›†åŠå…¶ä»–ç»„ã€‚ç»„æ”¯æŒåˆ›å»ºå‘½åç©ºé—´ï¼Œå®ç°æ•°æ®çš„é€»è¾‘ç»„ç»‡ã€‚

**æ•°æ®é›†(Dataset)**ï¼šå­˜å‚¨å®é™…æ•°æ®çš„åŸºæœ¬å•ä½ï¼Œå¯ä»¥çœ‹ä½œæ˜¯å¤šç»´æ•°ç»„ï¼Œæ”¯æŒå„ç§æ•°æ®ç±»å‹ã€‚

**å±æ€§(Attribute)**ï¼šå°å‹æ•°æ®é›†ï¼Œç”¨äºå­˜å‚¨æè¿°å…¶ä»–å¯¹è±¡çš„å…ƒæ•°æ®ï¼Œå¦‚å•ä½ã€åˆ›å»ºæ—¶é—´ç­‰ã€‚

### 2.2 æ•°æ®ç©ºé—´ä¸æ•°æ®ç±»å‹

HDF5çš„æ•°æ®æ¨¡å‹åŸºäºä¸¤ä¸ªæ ¸å¿ƒæ¦‚å¿µï¼š

**æ•°æ®ç±»å‹(Data Type)**ï¼šå®šä¹‰æ•°æ®çš„å­˜å‚¨æ ¼å¼ï¼ŒåŒ…æ‹¬æ•´æ•°ã€æµ®ç‚¹æ•°ã€å­—ç¬¦ä¸²ç­‰åŸºæœ¬ç±»å‹ï¼Œä»¥åŠå¤æ‚çš„å¤åˆç±»å‹ã€‚

**æ•°æ®ç©ºé—´(Data Space)**ï¼šå®šä¹‰æ•°æ®çš„ç»´åº¦å’Œå½¢çŠ¶ï¼Œä»æ ‡é‡ï¼ˆé›¶ç»´ï¼‰åˆ°Nç»´æ•°ç»„ã€‚

## 3. Pythonä¸­ä½¿ç”¨HDF5ï¼šh5pyå®æˆ˜æŒ‡å—

### 3.1 å®‰è£…ä¸åŸºç¡€æ–‡ä»¶æ“ä½œ

é¦–å…ˆå®‰è£…h5pyåº“ï¼š
```bash
pip install h5py
```

åŸºç¡€æ–‡ä»¶æ“ä½œï¼š
```python
import h5py
import numpy as np

# åˆ›å»ºHDF5æ–‡ä»¶
with h5py.File('example.h5', 'w') as f:
    # åˆ›å»ºæ•°æ®é›†
    data = np.random.random((100, 100))
    dataset = f.create_dataset('dataset1', data=data)
    
    # æ·»åŠ å±æ€§
    dataset.attrs['description'] = 'éšæœºæ•°æ•°æ®é›†'
    dataset.attrs['åˆ›å»ºæ—¥æœŸ'] = '2023-01-01'

# è¯»å–HDF5æ–‡ä»¶
with h5py.File('example.h5', 'r') as f:
    # è®¿é—®æ•°æ®é›†
    dataset = f['dataset1']
    print(f"æ•°æ®é›†å½¢çŠ¶ï¼š{dataset.shape}")
    print(f"æ•°æ®ç±»å‹ï¼š{dataset.dtype}")
    
    # è¯»å–å±æ€§
    if 'description' in dataset.attrs:
        print(f"æè¿°ï¼š{dataset.attrs['description']}")
```

### 3.2 åˆ›å»ºå’Œç®¡ç†ç»„

ç»„æ˜¯ç»„ç»‡æ•°æ®çš„æ ¸å¿ƒå®¹å™¨ï¼š
```python
with h5py.File('organized_data.h5', 'w') as f:
    # åˆ›å»ºç»„
    image_group = f.create_group('images')
    model_group = f.create_group('models')
    
    # åœ¨ç»„å†…åˆ›å»ºå­ç»„
    training_group = image_group.create_group('training')
    validation_group = image_group.create_group('validation')
    
    # åœ¨ç»„å†…åˆ›å»ºæ•°æ®é›†
    dummy_images = np.random.random((50, 64, 64, 3))
    training_group.create_dataset('cat_images', data=dummy_images)
    
    # éå†ç»„å†…å®¹
    def print_structure(name, obj):
        print(f"{name}: {type(obj).__name__}")
    
    f.visititems(print_structure)
```

### 3.3 é«˜çº§æ•°æ®é›†æ“ä½œ

#### 3.3.1 å‹ç¼©å’Œåˆ†å—å­˜å‚¨
å¯¹äºå¤§å‹æ•°æ®é›†ï¼Œå‹ç¼©å’Œåˆ†å—è‡³å…³é‡è¦ï¼š
```python
with h5py.File('compressed_data.h5', 'w') as f:
    # å¤§å‹æ•°æ®é›†ç¤ºä¾‹
    large_data = np.random.random((1000, 1000, 10))
    
    # å¯ç”¨å‹ç¼©å’Œåˆ†å—
    dataset = f.create_dataset('large_dataset', 
                             data=large_data,
                             compression='gzip',      # ä½¿ç”¨gzipå‹ç¼©
                             compression_opts=9,      # å‹ç¼©çº§åˆ«(1-9)
                             chunks=(100, 100, 1),   # åˆ†å—å¤§å°
                             shuffle=True)           # å¯ç”¨å­—èŠ‚æ´—ç‰Œ
    
    print(f"åŸå§‹æ•°æ®å¤§å°ï¼š{large_data.nbytes / (1024**2):.2f} MB")
    print(f"å‹ç¼©åå¤§å°ï¼š{dataset.id.get_storage_size() / (1024**2):.2f} MB")
```

#### 3.3.2 éƒ¨åˆ†I/Oæ“ä½œ
HDF5å…è®¸é«˜æ•ˆçš„éƒ¨åˆ†è¯»å–å’Œå†™å…¥ï¼š
```python
# åˆ›å»ºç¤ºä¾‹æ–‡ä»¶
with h5py.File('partial_io.h5', 'w') as f:
    large_array = np.arange(1000000).reshape(1000, 1000)
    f.create_dataset('big_data', data=large_array)

# éƒ¨åˆ†è¯»å–
with h5py.File('partial_io.h5', 'r') as f:
    dataset = f['big_data']
    
    # åªè¯»å–ç‰¹å®šåŒºåŸŸ
    subset = dataset[100:200, 300:400]  # è¯»å–100x100çš„å­åŒºåŸŸ
    print(f"å­é›†å½¢çŠ¶ï¼š{subset.shape}")
    
    # é€å—å¤„ç†å¤§å‹æ•°æ®é›†
    for i in range(0, dataset.shape[0], 100):
        chunk = dataset[i:i+100, :]      # æ¯æ¬¡è¯»å–100è¡Œ
        print(f"å¤„ç†å— {i}-{i+100}")
```

### 3.4 å±æ€§ç®¡ç†
å±æ€§æ˜¯å­˜å‚¨å…ƒæ•°æ®çš„å…³é”®ï¼š
```python
with h5py.File('attributes_demo.h5', 'w') as f:
    # åˆ›å»ºæ•°æ®é›†
    temperature_data = np.random.normal(25, 5, (365,))
    dataset = f.create_dataset('temperature', data=temperature_data)
    
    # æ·»åŠ å¤šç§ç±»å‹çš„å±æ€§
    dataset.attrs['unit'] = 'celsius'
    dataset.attrs['location'] = 'Beijing'
    dataset.attrs['year'] = 2023
    dataset.attrs['average'] = np.mean(temperature_data)
    
    # æ·»åŠ å¤æ‚å±æ€§ï¼ˆéœ€è¦åºåˆ—åŒ–ï¼‰
    import json
    metadata = {
        'sensor_id': 'temp_sensor_001',
        'calibration_date': '2023-01-15',
        'accuracy': 0.1
    }
    dataset.attrs['metadata'] = json.dumps(metadata)

# è¯»å–å±æ€§
with h5py.File('attributes_demo.h5', 'r') as f:
    dataset = f['temperature']
    
    print("æ‰€æœ‰å±æ€§ï¼š")
    for key in dataset.attrs:
        value = dataset.attrs[key]
        print(f"  {key}: {value}")
```

## 4. æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### 4.1 é€‰æ‹©åˆé€‚çš„å‹ç¼©å‚æ•°
```python
# ä¸åŒå‹ç¼©ç®—æ³•æ¯”è¾ƒ
def test_compression_algorithms():
    data = np.random.random((500, 500, 10))  # çº¦20MBæ•°æ®
    
    algorithms = [
        (None, {}),                           # æ— å‹ç¼©
        ('gzip', {'compression_opts': 6}),     # gzipé»˜è®¤
        ('gzip', {'compression_opts': 9}),     # gzipæœ€é«˜å‹ç¼©
        ('lzf', {}),                          # LZFå¿«é€Ÿå‹ç¼©
    ]
    
    for alg, opts in algorithms:
        with h5py.File(f'test_{alg}.h5', 'w') as f:
            dataset = f.create_dataset('data', data=data, compression=alg, **opts)
            size_mb = dataset.id.get_storage_size() / (1024**2)
            print(f"{alg}: {size_mb:.2f} MB")
```

### 4.2 åˆ†å—ç­–ç•¥ä¼˜åŒ–
åˆ†å—å¤§å°å½±å“I/Oæ€§èƒ½ï¼Œåº”æ ¹æ®è®¿é—®æ¨¡å¼ä¼˜åŒ–ï¼š
```python
def optimize_chunking():
    # æ¨¡æ‹Ÿä¸åŒåˆ†å—ç­–ç•¥
    data = np.zeros((10000, 10000))
    
    chunk_strategies = [
        (1000, 1000),    # å¤§åˆ†å—ï¼šé€‚åˆé¡ºåºè®¿é—®
        (100, 100),      # ä¸­ç­‰åˆ†å—ï¼šå¹³è¡¡é€‰æ‹©
        (10, 10),        # å°åˆ†å—ï¼šé€‚åˆéšæœºè®¿é—®
    ]
    
    for chunks in chunk_strategies:
        with h5py.File(f'chunks_{chunks[0]}.h5', 'w') as f:
            dataset = f.create_dataset('data', data=data, chunks=chunks)
            print(f"åˆ†å—{chunks}: åˆ›å»ºæˆåŠŸ")
```

## 5. å®é™…åº”ç”¨æ¡ˆä¾‹

### 5.1 æœºå™¨å­¦ä¹ æ•°æ®é›†å­˜å‚¨
```python
def save_ml_dataset(features, labels, filename):
    """ä¿å­˜æœºå™¨å­¦ä¹ æ•°æ®é›†åˆ°HDF5"""
    with h5py.File(filename, 'w') as f:
        # å­˜å‚¨ç‰¹å¾å’Œæ ‡ç­¾
        f.create_dataset('features', data=features, compression='gzip')
        f.create_dataset('labels', data=labels, compression='gzip')
        
        # å­˜å‚¨æ•°æ®é›†å…ƒæ•°æ®
        f.attrs['num_samples'] = features.shape[0]
        f.attrs['feature_dim'] = features.shape[1]
        f.attrs['num_classes'] = len(np.unique(labels))
        f.attrs['creation_date'] = str(np.datetime64('now'))

def load_ml_dataset(filename, split_ratio=0.8):
    """ä»HDF5åŠ è½½æœºå™¨å­¦ä¹ æ•°æ®é›†"""
    with h5py.File(filename, 'r') as f:
        features = f['features'][:]
        labels = f['labels'][:]
        
        # åˆ†å‰²æ•°æ®é›†
        split_idx = int(len(features) * split_ratio)
        train_data = (features[:split_idx], labels[:split_idx])
        test_data = (features[split_idx:], labels[split_idx:])
        
        return train_data, test_data
```

### 5.2 æ—¶é—´åºåˆ—æ•°æ®å­˜å‚¨
```python
def store_time_series_data(sensor_data, timestamps, filename):
    """å­˜å‚¨ä¼ æ„Ÿå™¨æ—¶é—´åºåˆ—æ•°æ®"""
    with h5py.File(filename, 'w') as f:
        # åˆ›å»ºå¤åˆæ•°æ®ç±»å‹ç”¨äºæ—¶é—´æˆ³å’Œæ•°å€¼
        dtype = np.dtype([
            ('timestamp', 'f8'),    # æµ®ç‚¹æ•°æ—¶é—´æˆ³
            ('value', 'f4')         # æµ®ç‚¹æ•°å€¼
        ])
        
        # åˆ›å»ºæ•°æ®é›†
        compound_data = np.zeros(len(sensor_data), dtype=dtype)
        compound_data['timestamp'] = timestamps
        compound_data['value'] = sensor_data
        
        dataset = f.create_dataset('sensor_readings', 
                                 data=compound_data,
                                 compression='gzip')
        
        # æ·»åŠ æŸ¥è¯¢ç´¢å¼•éœ€è¦çš„ä¿¡æ¯
        dataset.attrs['time_period_start'] = timestamps[0]
        dataset.attrs['time_period_end'] = timestamps[-1]
        dataset.attrs['sensor_id'] = 'temperature_sensor_001'
```

## 6. æ³¨æ„äº‹é¡¹ä¸æœ€ä½³å®è·µ

### 6.1 æ–‡ä»¶æ“ä½œå®‰å…¨
```python
# å®‰å…¨çš„æ–‡ä»¶æ“ä½œæ¨¡å¼
def safe_file_operations():
    try:
        # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç¡®ä¿æ–‡ä»¶æ­£ç¡®å…³é—­
        with h5py.File('data.h5', 'r') as f:
            data = f['dataset'][:]
        print("æ–‡ä»¶è¯»å–æˆåŠŸ")
    except IOError as e:
        print(f"æ–‡ä»¶æ“ä½œå¤±è´¥ï¼š{e}")
    except KeyError as e:
        print(f"æ•°æ®é›†ä¸å­˜åœ¨ï¼š{e}")

# å¤„ç†å·²æ‰“å¼€çš„æ–‡ä»¶ï¼ˆWindowsç³»ç»Ÿå¸¸è§é—®é¢˜ï¼‰
def handle_locked_file():
    import os
    filename = 'data.h5'
    
    # æ£€æŸ¥å¹¶åˆ é™¤é”å®šæ–‡ä»¶
    lock_file = filename + '.lock'
    if os.path.exists(lock_file):
        print("æ£€æµ‹åˆ°é”å®šæ–‡ä»¶ï¼Œå°è¯•æ¸…ç†...")
        try:
            os.remove(lock_file)
            print("é”å®šæ–‡ä»¶å·²æ¸…é™¤")
        except PermissionError:
            print("æ— æ³•åˆ é™¤é”å®šæ–‡ä»¶ï¼Œè¯·å…³é—­å ç”¨ç¨‹åº")
```

### 6.2 ç‰ˆæœ¬å…¼å®¹æ€§å¤„ç†
```python
def ensure_backward_compatibility():
    """ç¡®ä¿HDF5æ–‡ä»¶çš„å‘åå…¼å®¹æ€§"""
    with h5py.File('compatible.h5', 'w') as f:
        # ä½¿ç”¨å¹¿æ³›æ”¯æŒçš„æ•°æ®ç±»å‹
        f.create_dataset('data_int', data=np.array([1, 2, 3], dtype=np.int32))
        f.create_dataset('data_float', data=np.array([1.0, 2.0], dtype=np.float64))
        
        # é¿å…ä½¿ç”¨è¿‡äºæ–°çš„HDF5ç‰¹æ€§
        # ä½¿ç”¨ç¨³å®šçš„å‹ç¼©ç®—æ³•
        f.create_dataset('stable_data', data=np.random.random(100),
                        compression='gzip', compression_opts=6)
        
        # è®°å½•æ–‡ä»¶ç‰ˆæœ¬ä¿¡æ¯
        f.attrs['file_version'] = '1.0'
        f.attrs['creation_library'] = f'h5py {h5py.__version__}'
        f.attrs['recommended_h5py_version'] = '>=2.10.0'
```

## 7. å¯è§†åŒ–ä¸è°ƒè¯•å·¥å…·

### 7.1 ä½¿ç”¨HDFViewæŸ¥çœ‹æ–‡ä»¶
HDFGroupæä¾›çš„HDFViewå·¥å…·å¯ä»¥ç›´è§‚æŸ¥çœ‹HDF5æ–‡ä»¶ç»“æ„ï¼Œæ”¯æŒå…¨å¹³å°ä½¿ç”¨ã€‚

### 7.2 Pythonä¸­çš„ç®€å•å¯è§†åŒ–
```python
def visualize_hdf5_structure(filename):
    """å¯è§†åŒ–HDF5æ–‡ä»¶ç»“æ„"""
    def print_tree(name, obj, indent=0):
        spaces = '  ' * indent
        if isinstance(obj, h5py.Dataset):
            print(f"{spaces}ğŸ“Š Dataset: {name} {obj.shape} {obj.dtype}")
        elif isinstance(obj, h5py.Group):
            print(f"{spaces}ğŸ“ Group: {name}")
        
        # æ‰“å°å±æ€§
        if obj.attrs:
            attr_spaces = '  ' * (indent + 1)
            for key in obj.attrs:
                value = obj.attrs[key]
                print(f"{attr_spaces}ğŸ·ï¸  {key}: {value}")
    
    with h5py.File(filename, 'r') as f:
        f.visititems(print_tree)

# ä½¿ç”¨ç¤ºä¾‹
visualize_hdf5_structure('example.h5')
```

## æ€»ç»“

HDF5ä½œä¸ºå¤„ç†å¤§è§„æ¨¡ç§‘å­¦æ•°æ®çš„**è¡Œä¸šæ ‡å‡†**ï¼Œæä¾›äº†é«˜æ•ˆã€çµæ´»çš„æ•°æ®ç®¡ç†è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡æŒæ¡å…¶åˆ†å±‚ç»“æ„ã€å‹ç¼©æŠ€æœ¯å’Œéƒ¨åˆ†I/Oæ“ä½œï¼Œä½ å¯ä»¥åœ¨å„ç§æ•°æ®å¯†é›†å‹åº”ç”¨ä¸­æ¸¸åˆƒæœ‰ä½™ã€‚

**å…³é”®è¦ç‚¹å›é¡¾**ï¼š
- HDF5çš„å±‚æ¬¡ç»“æ„å®Œç¾åŒ¹é…å¤æ‚æ•°æ®ç»„ç»‡éœ€æ±‚
- å‹ç¼©å’Œåˆ†å—æŠ€æœ¯å¤§å¹…æå‡å­˜å‚¨æ•ˆç‡
- å±æ€§ç³»ç»Ÿä¸ºæ•°æ®æ·»åŠ ä¸°å¯Œçš„å…ƒæ•°æ®ä¸Šä¸‹æ–‡
- éƒ¨åˆ†I/Oæ“ä½œæ”¯æŒé«˜æ•ˆå¤„ç†è¶…å¤§å‹æ•°æ®é›†

æ— è®ºæ˜¯ç§‘å­¦ç ”ç©¶ã€é‡‘èåˆ†æè¿˜æ˜¯æœºå™¨å­¦ä¹ ï¼ŒHDF5éƒ½èƒ½ä¸ºä½ çš„æ•°æ®ç®¡ç†æä¾›åšå® foundationã€‚ç°åœ¨å°±å¼€å§‹åœ¨ä½ çš„é¡¹ç›®ä¸­å®è·µè¿™äº›æŠ€å·§å§ï¼