#cbam #attention #cnn 

  

> CBAMï¼ˆConvolutional Block Attention Moduleï¼‰æ˜¯ä¸€ç§è½»é‡çº§ä½†æœ‰æ•ˆçš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œç”± **é€šé“æ³¨æ„åŠ›ï¼ˆChannel Attentionï¼‰** å’Œ **ç©ºé—´æ³¨æ„åŠ›ï¼ˆSpatial Attentionï¼‰** ä¸¤éƒ¨åˆ†ç»„æˆã€‚å®ƒå¯ä»¥åµŒå…¥åˆ°å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä¸­ï¼Œä»¥å¢å¼ºç‰¹å¾æå–èƒ½åŠ›ï¼Œæé«˜å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹å’Œè¯­ä¹‰åˆ†å‰²ç­‰ä»»åŠ¡çš„æ€§èƒ½ã€‚
> 
> [è®ºæ–‡](https://arxiv.org/abs/1807.06521) [Non-officialä»£ç ä»“åº“](https://github.com/luuuyi/CBAM.PyTorch)


# CBAM çš„æ•´ä½“ç»“æ„
CBAM ç”±ä¸¤ä¸ªä¸²è”çš„å­æ¨¡å—æ„æˆï¼š

1. **é€šé“æ³¨æ„åŠ›ï¼ˆChannel Attention Module, CAMï¼‰**

2. **ç©ºé—´æ³¨æ„åŠ›ï¼ˆSpatial Attention Module, SAMï¼‰**
![[CBAMç»“æ„.png]]

åœ¨ CBAM ä¸­ï¼Œè¾“å…¥ç‰¹å¾å›¾ $\mathbf{F} \in \mathbb{R}^{C \times H \times W}$ å…ˆç»è¿‡ **é€šé“æ³¨æ„åŠ›æ¨¡å—**ï¼Œç„¶åé€šè¿‡ **ç©ºé—´æ³¨æ„åŠ›æ¨¡å—**ï¼Œæœ€ç»ˆå¾—åˆ°å¢å¼ºçš„ç‰¹å¾è¡¨ç¤ºã€‚
**è®¡ç®—æµç¨‹ï¼š**

$$

\mathbf{Fâ€™} = \text{CAM}(\mathbf{F}) \cdot \mathbf{F}

$$

$$

\mathbf{Fâ€™â€™} = \text{SAM}(\mathbf{Fâ€™}) \cdot \mathbf{Fâ€™}

$$

  

å…¶ä¸­ï¼š

â€¢ $\mathbf{F}$ æ˜¯è¾“å…¥ç‰¹å¾å›¾

â€¢ $\mathbf{Fâ€™}$ æ˜¯ç»è¿‡é€šé“æ³¨æ„åŠ›æ¨¡å—åçš„ç‰¹å¾å›¾

â€¢ $\mathbf{Fâ€™â€™}$ æ˜¯æœ€ç»ˆè¾“å‡ºçš„ç‰¹å¾å›¾

â€¢ CAM(Â·) å’Œ SAM(Â·) åˆ†åˆ«è¡¨ç¤ºé€šé“æ³¨æ„åŠ›å’Œç©ºé—´æ³¨æ„åŠ›

---

**2. é€šé“æ³¨æ„åŠ›æœºåˆ¶ï¼ˆChannel Attentionï¼‰**

  

é€šé“æ³¨æ„åŠ›æ¨¡å—ç”¨äº**æ•æ‰å…¨å±€é€šé“å…³ç³»**ï¼Œé€šè¿‡è‡ªé€‚åº”åœ°è°ƒæ•´å„é€šé“çš„é‡è¦æ€§ï¼Œæ¥æé«˜ç½‘ç»œå¯¹å…³é”®é€šé“çš„å…³æ³¨ã€‚

  

**è®¡ç®—æ­¥éª¤**

1. **å…¨å±€å¹³å‡æ± åŒ–ï¼ˆGAP, Global Average Poolingï¼‰** å’Œ **å…¨å±€æœ€å¤§æ± åŒ–ï¼ˆGMP, Global Max Poolingï¼‰** è®¡ç®—é€šé“ç»´åº¦ä¸Šçš„å…¨å±€ç‰¹å¾ï¼š

$$

\mathbf{F}_{\text{avg}}^c = \frac{1}{H \times W} \sum_{i=1}^{H} \sum_{j=1}^{W} \mathbf{F}(i,j)

$$

$$

\mathbf{F}_{\text{max}}^c = \max_{i \in H, j \in W} \mathbf{F}(i,j)

$$

å…¶ä¸­ï¼Œ$\mathbf{F}_{\text{avg}}^c, \mathbf{F}_{\text{max}}^c \in \mathbb{R}^{C}$ã€‚

2. **å…±äº« MLPï¼ˆMulti-Layer Perceptronï¼‰** è¿›è¡Œç‰¹å¾èåˆï¼š

â€¢ MLP ç”±ä¸¤å±‚å…¨è¿æ¥å±‚ç»„æˆï¼š

$$

MLP(x) = W_2 \sigma(W_1 x)

$$

â€¢ è®¡ç®—ä¸¤ä¸ªæ± åŒ–ç‰¹å¾çš„æ³¨æ„åŠ›æƒé‡ï¼š

$$

\mathbf{M}_c = \sigma(\text{MLP}(\mathbf{F}_{\text{avg}}) + \text{MLP}(\mathbf{F}_{\text{max}}))

$$

3. **Sigmoid å½’ä¸€åŒ–** å¹¶ä¸åŸç‰¹å¾ç›¸ä¹˜ï¼š

$$

\mathbf{Fâ€™} = \mathbf{M}_c \cdot \mathbf{F}

$$

  

**é€šé“æ³¨æ„åŠ›ç»“æ„**

â€¢ è¾“å…¥ï¼š$\mathbf{F} \in \mathbb{R}^{C \times H \times W}$

â€¢ ç»è¿‡ GAP å’Œ GMPï¼Œå¾—åˆ°ä¸¤ä¸ªé€šé“æè¿°

â€¢ ç»è¿‡å…±äº« MLP å¹¶ç›¸åŠ 

â€¢ é€šè¿‡ Sigmoid æ¿€æ´»

â€¢ ä¹˜å›åŸç‰¹å¾ $\mathbf{F}$

---

**3. ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶ï¼ˆSpatial Attentionï¼‰**

  

é€šé“æ³¨æ„åŠ›å¢å¼ºäº†é€šé“ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œè€Œ**ç©ºé—´æ³¨æ„åŠ›å…³æ³¨çš„æ˜¯æ¯ä¸ªä½ç½®çš„é‡è¦æ€§**ï¼Œå³å“ªäº›åŒºåŸŸæ›´é‡è¦ã€‚

  

**è®¡ç®—æ­¥éª¤**

1. **é€šé“ç»´åº¦çš„æœ€å¤§æ± åŒ–å’Œå¹³å‡æ± åŒ–**ï¼Œåœ¨ç©ºé—´ç»´åº¦ä¸Šè¿›è¡Œï¼š

â€¢ å¯¹ç‰¹å¾å›¾çš„é€šé“ç»´åº¦åšæœ€å¤§æ± åŒ–å’Œå¹³å‡æ± åŒ–ï¼š

$$

\mathbf{F}_{\text{avg}}^s = \frac{1}{C} \sum_{k=1}^{C} \mathbf{F}_k_

_$$_

_$$_

_\mathbf{F}_{\text{max}}^s = \max_{k \in C} \mathbf{F}_k

$$

â€¢ å…¶ä¸­ï¼Œ$\mathbf{F}_{\text{avg}}^s, \mathbf{F}_{\text{max}}^s \in \mathbb{R}^{1 \times H \times W}$ã€‚

2. **å°†ä¸¤ä¸ªç‰¹å¾æ‹¼æ¥ï¼ˆConcatï¼‰ï¼Œå†é€šè¿‡ä¸€ä¸ª $7 \times 7$ çš„å·ç§¯å±‚è¿›è¡Œç‰¹å¾æå–**ï¼š

$$

\mathbf{M}_s = \sigma(\text{Conv}_{7 \times 7}([\mathbf{F}_{\text{avg}}^s; \mathbf{F}_{\text{max}}^s]))

$$

3. **å½’ä¸€åŒ–å¹¶ä¹˜å›åŸç‰¹å¾**ï¼š

$$

\mathbf{Fâ€™â€™} = \mathbf{M}_s \cdot \mathbf{Fâ€™}

$$

  

**ç©ºé—´æ³¨æ„åŠ›ç»“æ„**

â€¢ è¾“å…¥ï¼š$\mathbf{Fâ€™} \in \mathbb{R}^{C \times H \times W}$

â€¢ ç»è¿‡é€šé“æ–¹å‘çš„ GAP å’Œ GMP

â€¢ ç»è¿‡ $7 \times 7$ å·ç§¯

â€¢ é€šè¿‡ Sigmoid æ¿€æ´»

â€¢ ä¹˜å›è¾“å…¥ç‰¹å¾ $\mathbf{Fâ€™}$

---

**4. CBAM çš„ä¼˜ç‚¹**

1. **è½»é‡çº§**ï¼šç›¸æ¯”äº SEï¼ˆSqueeze-and-Excitationï¼‰æ¨¡å—ï¼ŒCBAM è®¡ç®—é‡æ›´å°ã€‚

2. **æ›´å¼ºçš„ç‰¹å¾è¡¨è¾¾èƒ½åŠ›**ï¼šåŒæ—¶åˆ©ç”¨äº†é€šé“æ³¨æ„åŠ›å’Œç©ºé—´æ³¨æ„åŠ›ï¼Œæ›´å…¨é¢åœ°å¢å¼ºå…³é”®ç‰¹å¾ã€‚

3. **å®¹æ˜“åµŒå…¥ CNN ç»“æ„**ï¼šå¯ä»¥æ— ç¼é›†æˆåˆ°ä¸»æµ CNNï¼Œå¦‚ ResNetã€VGG ç­‰ï¼Œæé«˜åˆ†ç±»ã€æ£€æµ‹ç­‰ä»»åŠ¡çš„æ€§èƒ½ã€‚

---

**5. PyTorch å®ç° CBAM**

```python
import torch
import torch.nn as nn

class ChannelAttention(nn.Module):
    def __init__(self, in_channels, ratio=16):
        super(ChannelAttention, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)

        self.mlp = nn.Sequential(
            nn.Linear(in_channels, in_channels // ratio, bias=False),
            nn.ReLU(),
            nn.Linear(in_channels // ratio, in_channels, bias=False)
        )
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = self.mlp(self.avg_pool(x).view(x.shape[0], -1))
        max_out = self.mlp(self.max_pool(x).view(x.shape[0], -1))
        out = avg_out + max_out
        return self.sigmoid(out).view(x.shape[0], x.shape[1], 1, 1) * x

class SpatialAttention(nn.Module):
    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()
        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        out = torch.cat([avg_out, max_out], dim=1)
        return self.sigmoid(self.conv(out)) * x

class CBAM(nn.Module):
    def __init__(self, in_channels, ratio=16, kernel_size=7):
        super(CBAM, self).__init__()
        self.channel_attention = ChannelAttention(in_channels, ratio)
        self.spatial_attention = SpatialAttention(kernel_size)

    def forward(self, x):
        x = self.channel_attention(x)
        x = self.spatial_attention(x)
        return x
```

  

---

**6. æ€»ç»“**

â€¢ **CBAM = é€šé“æ³¨æ„åŠ› + ç©ºé—´æ³¨æ„åŠ›**

â€¢ å…ˆæ‰§è¡Œé€šé“æ³¨æ„åŠ›ï¼Œå†æ‰§è¡Œç©ºé—´æ³¨æ„åŠ›

â€¢ è½»é‡ã€é«˜æ•ˆï¼Œé€‚ç”¨äºå„ç§ CNN ä»»åŠ¡

---

ä»¥ä¸Šå°±æ˜¯ **CBAMï¼ˆå·ç§¯å—æ³¨æ„åŠ›æ¨¡å—ï¼‰** çš„è¯¦ç»†ä»‹ç»åŠ PyTorch å®ç° ğŸš€ã€‚