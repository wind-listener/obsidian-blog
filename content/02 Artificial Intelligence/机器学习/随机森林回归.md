
# 随机森林回归算法详解：原理、参数、代码与评估

## 一、算法原理
随机森林回归（Random Forest Regression, RFR）是一种基于**集成学习**的回归算法，通过组合多个决策树的预测结果提升模型的泛化能力。其核心思想可概括为以下三点：

### 1. **集成学习框架**
- 通过**Bootstrap抽样**从原始数据集中有放回地抽取$N$个样本，生成$M$个子训练集。
- 每个子训练集独立训练一棵CART回归树，形成“森林”。

### 2. **双重随机性**
- **样本随机性**：每棵树仅使用约63%的原始样本（剩余37%为袋外数据OOB）。
- **特征随机性**：每棵树分裂时仅随机选择$F \leq M$个特征（$M$为总特征数），典型选择方式为$\sqrt{M}$或$\log_2 M$。

### 3. **预测机制**
- 每棵树的预测结果为叶子节点样本的均值。
- 最终预测值为所有决策树预测结果的均值：
  $$
  \hat{y} = \frac{1}{T} \sum_{t=1}^T f_t(x)
  $$
  其中$T$为树的数量，$f_t(x)$为第$t$棵树的预测值。

---

## 二、关键超参数解析
以下是随机森林回归的关键参数及其影响：

| 参数名称              | 作用描述                                                                 | 建议值/选项                          |
|-----------------------|--------------------------------------------------------------------------|--------------------------------------|
| `n_estimators`        | 森林中决策树的数量，增加可提升稳定性但计算成本增加                      | 100-500（需交叉验证调整）   |
| `max_features`        | 每棵树分裂时考虑的最大特征数，控制特征随机性                            | "sqrt"（默认）、"log2"、整数/浮点数 |
| `max_depth`           | 树的最大深度，限制模型复杂度                                            | 5-30（或None不限制）        |
| `min_samples_split`   | 节点分裂所需最小样本数，防止过拟合                                       | 默认2，回归任务建议5-10      |
| `min_samples_leaf`    | 叶子节点最小样本数，平滑模型预测                                         | 默认1，回归任务建议5-10     |
| `bootstrap`           | 是否启用Bootstrap抽样，True时可计算OOB误差                              | True（默认）                |
| `oob_score`           | 使用袋外数据评估模型性能                                                | True（推荐）               |

---

## 三、训练代码示例（Python/scikit-learn）
```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import pandas as pd

# 1. 数据加载与预处理
data = pd.read_csv('dataset.csv')
X = data.drop('target', axis=1)
y = data['target']

# 2. 划分训练集与测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 3. 模型初始化与训练
rf = RandomForestRegressor(
    n_estimators=200,
    max_features='sqrt',
    max_depth=10,
    min_samples_leaf=5,
    bootstrap=True,
    oob_score=True,
    random_state=42
)
rf.fit(X_train, y_train)

# 4. 预测与评估
y_pred = rf.predict(X_test)
print(f"MSE: {mean_squared_error(y_test, y_pred):.2f}")
print(f"R²: {r2_score(y_test, y_pred):.2f}")
print(f"OOB Score: {rf.oob_score_:.2f}")

# 5. 特征重要性分析
feature_importances = pd.Series(rf.feature_importances_, index=X.columns)
feature_importances.sort_values(ascending=False).plot(kind='bar')
```

---

## 四、评估指标
### 1. **回归性能指标**
- **均方误差（MSE）**：
  $$
  \text{MSE} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2
  $$
- **均方根误差（RMSE）**：
  $$
  \text{RMSE} = \sqrt{\text{MSE}}
  $$
- **决定系数（R²）**：
  $$
  R^2 = 1 - \frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{\sum_{i=1}^n (y_i - \bar{y})^2}
  $$

### 2. **模型诊断工具**
- **袋外误差（OOB Error）**：通过未参与训练的37%样本计算误差，无需额外验证集。
- **特征重要性**：基于特征在树分裂中的贡献度排序。

---

## 五、调参策略与注意事项
### 1. **系统调参方法**
- **网格搜索（GridSearchCV）**：遍历预设参数组合寻找最优解。
- **随机搜索（RandomizedSearchCV）**：适用于高维参数空间。

### 2. **实践建议**
- **处理缺失值**：随机森林对缺失值鲁棒，但建议使用插补（如KNN）提升性能。
- **类别特征编码**：需转换为数值型（如独热编码）。
- **并行计算**：设置`n_jobs=-1`利用多核加速训练。


## 参考文献
