对比原始图片/视频和重建图片/视频是计算机视觉、图像处理和视频压缩等领域非常核心的任务，有时还没有原始信号可供比较，这是更具挑战性的场景。

根据原理和关注点分为以下几大类：
# 全参考图像质量评估

这类指标需要完整的原始图像（参考图像）和重建图像（失真图像）进行计算。

#### 1. 基于像素差异的指标（简单直观，但与人类感知相关性较弱）
*   **均方误差 - Mean Squared Error**
    *   **公式**：$MSE = \frac{1}{mn}\sum_{i=0}^{m-1}\sum_{j=0}^{n-1}[I(i,j) - K(i,j)]^2$
    *   **说明**：计算两幅图像像素值之差的平方的平均数。值越**小**越好，0表示完全相同。
    *   **优点**：计算简单，有明确的物理意义。
    *   **缺点**：与人类主观感受的一致性较差。例如，轻微的图像偏移会导致MSE暴增，但人眼可能不易察觉；而一些人眼敏感的失真（如边缘模糊）可能MSE变化不大。

*   **[[峰值信噪比 PSNR|峰值信噪比]] - Peak Signal-to-Noise Ratio**
    *   **公式**：$PSNR = 10 \cdot \log_{10}(\frac{MAX_I^2}{MSE})$
    *   **说明**：基于MSE，MAX_I是图像像素的最大可能值（例如，8位图像为255）。值越**大**越好，无限大表示完全相同。
    *   **优点**：计算非常快速、简单，是最广泛使用的指标之一。
    *   **缺点**：同样与人类视觉系统（HVS）的感知特性不符。通常认为PSNR在30dB以上时，图像质量就非常好了。

#### 2. 基于结构相似性的指标（更符合人类视觉感知）
*   **[[SSIM：结构相似性图像质量评估指标|结构相似性指数]] - Structural Similarity Index**
    *   **公式**：比较两幅图像的**亮度（luminance）**、**对比度（contrast）** 和**结构（structure）** 三个方面的相似性，最终得到一个0到1之间的分数。
    *   **说明**：SSIM试图模拟人类视觉系统对结构信息的敏感度。值越**大**越好，1表示完全相同。
    *   **优点**：比PSNR/MSE更符合人眼主观感受，已成为图像质量评估的工业标准之一。
    *   **变种**：**MS-SSIM** 在多尺度上计算SSIM，能更好地评估不同观看距离下的感知质量。

#### 3. 基于感知和信息保真度的指标（更先进的感知指标）
*   **学习感知图像块相似度 - Learned Perceptual Image Patch Similarity**
    *   **说明**：利用预训练好的深度卷积神经网络（如VGG、AlexNet）来提取图像的特征，然后计算特征图之间的相似度。它衡量的是两幅图像在“感知特征空间”中的距离。
    *   **优点**：与人类主观评价的相关性**极高**，能很好地处理纹理、风格等高级语义信息的差异。值越**小**越好，0表示完全相同。
    *   **缺点**：计算量相对较大。

# 全参考视频质量评估

视频评估除了单帧图像质量，还需考虑时间域（帧间）的连贯性。

*   **PSNR和SSIM的扩展**：
    *   可以逐帧计算图像的PSNR或SSIM，然后求整个视频序列的**平均值**，得到**PSNR-AVG**或**SSIM-AVG**。这是最基础的方法。
*   **视频多方法评估融合 - Video Multimethod Assessment Fusion**
    *   **说明**：Netflix开发并开源的一个指标，旨在预测人类对视频质量的主观评分（MOS）。它结合了多个基础指标（包括PSNR、SSIM、时域指标等），通过机器学习模型融合成一个最终分数。
    *   **优点**：是目前与主观评价相关性最好的客观视频质量指标之一，被工业界广泛采用。
    *   **缺点**：计算复杂。

# 无参考图像/视频质量评估

当原始参考图像/视频不可用时（这是更常见且更具挑战性的场景），需要使用无参考指标。

*   **自然图像质量评估器 - Natural Image Quality Evaluator**
    *   **说明**：基于自然场景统计（NSS）模型，它假设“自然”的图像具有一定的统计规律，而失真会破坏这种规律。NIQE通过衡量这种破坏程度来评估质量。
    *   **优点**：完全无参考，不需要原始图像。
    *   **缺点**：准确性通常不如全参考指标。

*   **基于深度学习的无参考模型**：
    *   近年来出现了很多基于CNN或Transformer的无参考质量评估模型，它们在海量的（图像，人工评分）数据上进行训练，可以直接预测图像的感知质量分数。这些是当前的研究热点。
#  针对特定任务的指标

*   **生成模型（如GANs）**：
    *   **初始分数 - Inception Score**: 衡量生成图像的多样性和清晰度。
    *   **Fréchet 初始距离 - Fréchet Inception Distance**: 比较生成图像和真实图像在特征空间中的分布距离。值越**小**越好。

# 总结与选择建议

| 指标名称             | 类型  | 需求  | 优点             | 缺点       | 适用场景                |
| :--------------- | :-- | :-- | :------------- | :------- | :------------------ |
| **MSE/PSNR**     | 全参考 | 原图  | 计算快，普及度高       | 与感知相关性差  | 初版快速验证，算法基础对比       |
| **SSIM/MS-SSIM** | 全参考 | 原图  | 比PSNR更符合人眼     | 对某些失真不敏感 | 图像压缩、传输质量评估         |
| **LPIPS**        | 全参考 | 原图  | **与感知高度相关**    | 计算量较大    | 超分辨率、图像修复、风格迁移等感知任务 |
| **VMAF**         | 全参考 | 原视频 | **视频感知质量黄金标准** | 计算复杂     | 视频编码、流媒体服务质量评估      |
| **NIQE**         | 无参考 | 无   | 无需原图           | 准确性一般    | 无法获取原图时的质量监控        |
|                  |     |     |                |          |                     |

**最佳实践**：
对于严肃的研究或工程项目，**不应只依赖一个指标**。通常的建议是：
1.  **同时报告PSNR和SSIM**：因为它们易于理解且被广泛接受。
2.  **强烈推荐同时报告LPIPS**：因为它能更好地反映算法在“看起来是否相似”方面的性能。
3.  **如果是视频，VMAF是目前的最佳选择**。
4.  **最终一定要配合主观评价（人眼观看）**：任何客观指标都是对主观感受的模拟，最终都需要人的验证。